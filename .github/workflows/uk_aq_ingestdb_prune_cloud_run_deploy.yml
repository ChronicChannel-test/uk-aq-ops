name: Deploy IngestDB Prune Cloud Run Service

on:
  push:
    branches:
      - main
    paths:
      - "src/server.mjs"
      - "Dockerfile"
      - "package.json"
      - "package-lock.json"
      - ".github/workflows/uk_aq_ingestdb_prune_cloud_run_deploy.yml"
  workflow_dispatch:

concurrency:
  group: uk-aq-ingestdb-prune-deploy
  cancel-in-progress: false

jobs:
  deploy-ingestdb-prune-service:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write
    env:
      PROJECT_ID: ${{ vars.GCP_PROJECT_ID }}
      REGION: ${{ vars.GCP_REGION || 'europe-west2' }}
      REPO: ${{ vars.GCP_ARTIFACT_REPO || 'uk-aq' }}
      SERVICE_NAME: ${{ vars.GCP_INGESTDB_PRUNE_SERVICE_NAME || vars.INGESTDB_PRUNE_SERVICE_NAME || 'uk-aq-ingestdb-prune-service' }}
      RESOURCE_LABEL: uk-aq-ingestdb-prune-service
      INGESTDB_PRUNE_RUNTIME_SERVICE_ACCOUNT: ${{ vars.GCP_INGESTDB_PRUNE_SERVICE_ACCOUNT || vars.GCP_OPS_JOB_SERVICE_ACCOUNT || vars.INGESTDB_PRUNE_RUNTIME_SERVICE_ACCOUNT || '' }}
      SERVICE_TIMEOUT_SECONDS: ${{ vars.GCP_INGESTDB_PRUNE_SERVICE_TIMEOUT_SECONDS || vars.INGESTDB_PRUNE_SERVICE_TIMEOUT_SECONDS || '900' }}
      SERVICE_CPU: ${{ vars.GCP_INGESTDB_PRUNE_SERVICE_CPU || vars.INGESTDB_PRUNE_SERVICE_CPU || '1' }}
      SERVICE_MEMORY: ${{ vars.GCP_INGESTDB_PRUNE_SERVICE_MEMORY || vars.INGESTDB_PRUNE_SERVICE_MEMORY || '256Mi' }}
      SERVICE_CONCURRENCY: ${{ vars.GCP_INGESTDB_PRUNE_SERVICE_CONCURRENCY || vars.INGESTDB_PRUNE_SERVICE_CONCURRENCY || '1' }}
      SERVICE_MAX_INSTANCES: ${{ vars.GCP_INGESTDB_PRUNE_SERVICE_MAX_INSTANCES || vars.INGESTDB_PRUNE_SERVICE_MAX_INSTANCES || '1' }}
      SERVICE_MIN_INSTANCES: ${{ vars.GCP_INGESTDB_PRUNE_SERVICE_MIN_INSTANCES || vars.INGESTDB_PRUNE_SERVICE_MIN_INSTANCES || '0' }}
      SCHEDULER_ENABLED: ${{ vars.GCP_INGESTDB_PRUNE_SCHEDULER_ENABLED || vars.INGESTDB_PRUNE_SCHEDULER_ENABLED || 'true' }}
      SCHEDULER_JOB_NAME: ${{ vars.GCP_INGESTDB_PRUNE_SCHEDULER_JOB_NAME || vars.INGESTDB_PRUNE_SCHEDULER_JOB_NAME || 'uk-aq-ingestdb-prune-daily' }}
      SCHEDULER_CRON: ${{ vars.GCP_INGESTDB_PRUNE_SCHEDULER_CRON || vars.INGESTDB_PRUNE_SCHEDULER_CRON || '0 2 * * *' }}
      SCHEDULER_TIMEZONE: ${{ vars.GCP_INGESTDB_PRUNE_SCHEDULER_TIMEZONE || vars.INGESTDB_PRUNE_SCHEDULER_TIMEZONE || 'Etc/UTC' }}
      SCHEDULER_MAX_RETRY_ATTEMPTS: ${{ vars.GCP_INGESTDB_PRUNE_SCHEDULER_MAX_RETRY_ATTEMPTS || vars.INGESTDB_PRUNE_SCHEDULER_MAX_RETRY_ATTEMPTS || '0' }}
      INGESTDB_PRUNE_SCHEDULER_SERVICE_ACCOUNT: ${{ vars.GCP_INGESTDB_PRUNE_SCHEDULER_SERVICE_ACCOUNT || vars.INGESTDB_PRUNE_SCHEDULER_SERVICE_ACCOUNT || '' }}
      SUPABASE_URL: ${{ vars.SUPABASE_URL }}
      HISTORY_SUPABASE_URL: ${{ vars.HISTORY_SUPABASE_URL }}
      INGESTDB_PRUNE_DRY_RUN: ${{ vars.INGESTDB_PRUNE_DRY_RUN || 'true' }}
      INGESTDB_RETENTION_DAYS: ${{ vars.INGESTDB_RETENTION_DAYS || '7' }}
      MAX_HOURS_PER_RUN: ${{ vars.INGESTDB_PRUNE_MAX_HOURS_PER_RUN || '48' }}
      DELETE_BATCH_SIZE: ${{ vars.INGESTDB_PRUNE_DELETE_BATCH_SIZE || '50000' }}
      MAX_DELETE_BATCHES_PER_HOUR: ${{ vars.INGESTDB_PRUNE_MAX_DELETE_BATCHES_PER_HOUR || '10' }}
      REPAIR_ONE_MISMATCH_BUCKET: ${{ vars.INGESTDB_PRUNE_REPAIR_ONE_MISMATCH_BUCKET || 'true' }}
      REPAIR_BUCKET_OUTBOX_CHUNK_SIZE: ${{ vars.INGESTDB_PRUNE_REPAIR_BUCKET_OUTBOX_CHUNK_SIZE || '1000' }}
      FLUSH_CLAIM_BATCH_LIMIT: ${{ vars.INGESTDB_PRUNE_FLUSH_CLAIM_BATCH_LIMIT || '20' }}
      MAX_FLUSH_BATCHES: ${{ vars.INGESTDB_PRUNE_MAX_FLUSH_BATCHES || '30' }}
      SB_SECRET_KEY_SECRET_NAME: ${{ vars.SB_SECRET_KEY_SECRET_NAME || 'SB_SECRET_KEY' }}
      HISTORY_SECRET_KEY_SECRET_NAME: ${{ vars.HISTORY_SECRET_KEY_SECRET_NAME || 'HISTORY_SECRET_KEY' }}
      DROPBOX_APP_KEY_SECRET_NAME: ${{ vars.DROPBOX_APP_KEY_SECRET_NAME || '' }}
      DROPBOX_APP_SECRET_SECRET_NAME: ${{ vars.DROPBOX_APP_SECRET_SECRET_NAME || '' }}
      DROPBOX_REFRESH_TOKEN_SECRET_NAME: ${{ vars.DROPBOX_REFRESH_TOKEN_SECRET_NAME || '' }}
      UK_AQ_DROPBOX_ROOT: ${{ vars.UK_AQ_DROPBOX_ROOT || '' }}
      UK_AIR_ERROR_DROPBOX_FOLDER: ${{ vars.UK_AIR_ERROR_DROPBOX_FOLDER || '/error_log' }}
      UK_AIR_ERROR_DROPBOX_ALLOWED_SUPABASE_URL: ${{ vars.UK_AIR_ERROR_DROPBOX_ALLOWED_SUPABASE_URL || '' }}
      GCP_WORKLOAD_IDENTITY_PROVIDER: ${{ vars.GCP_WORKLOAD_IDENTITY_PROVIDER }}
      GCP_SERVICE_ACCOUNT: ${{ vars.GCP_SERVICE_ACCOUNT }}
      GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Validate required project config
        run: |
          set -euo pipefail
          if [ -z "${PROJECT_ID:-}" ]; then
            echo "Missing required repo variable: GCP_PROJECT_ID"
            exit 1
          fi
          if [ -z "${SUPABASE_URL:-}" ]; then
            echo "Missing required repo variable: SUPABASE_URL"
            exit 1
          fi
          if [ -z "${HISTORY_SUPABASE_URL:-}" ]; then
            echo "Missing required repo variable: HISTORY_SUPABASE_URL"
            exit 1
          fi

      - name: Resolve runtime and scheduler service accounts
        id: service_accounts
        run: |
          set -euo pipefail

          runtime_sa="${INGESTDB_PRUNE_RUNTIME_SERVICE_ACCOUNT:-}"
          if [ -z "${runtime_sa}" ]; then
            runtime_sa="uk-aq-ops-job@${PROJECT_ID}.iam.gserviceaccount.com"
          fi
          if [[ "${runtime_sa}" == *"-compute@developer.gserviceaccount.com" ]]; then
            echo "Invalid runtime service account: default compute service account is not allowed."
            exit 1
          fi

          scheduler_sa="${INGESTDB_PRUNE_SCHEDULER_SERVICE_ACCOUNT:-${runtime_sa}}"
          if [[ "${scheduler_sa}" == *"-compute@developer.gserviceaccount.com" ]]; then
            echo "Invalid scheduler service account: default compute service account is not allowed."
            exit 1
          fi

          echo "runtime_sa=${runtime_sa}" >> "${GITHUB_OUTPUT}"
          echo "scheduler_sa=${scheduler_sa}" >> "${GITHUB_OUTPUT}"

      - name: Validate Google auth config
        run: |
          set -euo pipefail
          if [ -n "${GCP_WORKLOAD_IDENTITY_PROVIDER:-}" ] && [ -n "${GCP_SERVICE_ACCOUNT:-}" ]; then
            echo "Using Workload Identity Federation auth via existing deploy SA."
          elif [ -n "${GCP_SA_KEY:-}" ]; then
            echo "Using Service Account key auth fallback."
          else
            echo "Missing Google auth config."
            echo "Set either:"
            echo "1) GCP_WORKLOAD_IDENTITY_PROVIDER + GCP_SERVICE_ACCOUNT"
            echo "or"
            echo "2) GCP_SA_KEY"
            exit 1
          fi

      - name: Authenticate to Google Cloud (WIF)
        if: ${{ env.GCP_WORKLOAD_IDENTITY_PROVIDER != '' && env.GCP_SERVICE_ACCOUNT != '' }}
        uses: google-github-actions/auth@c200f3691d83b41bf9bbd8638997a462592937ed
        with:
          workload_identity_provider: ${{ env.GCP_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ env.GCP_SERVICE_ACCOUNT }}

      - name: Authenticate to Google Cloud (Service Account Key fallback)
        if: ${{ env.GCP_WORKLOAD_IDENTITY_PROVIDER == '' || env.GCP_SERVICE_ACCOUNT == '' }}
        uses: google-github-actions/auth@c200f3691d83b41bf9bbd8638997a462592937ed
        with:
          credentials_json: ${{ env.GCP_SA_KEY }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@e427ad8a34f8676edf47cf7d7925499adf3eb74f

      - name: Configure gcloud project
        run: |
          set -euo pipefail
          gcloud config set project "${PROJECT_ID}"
          gcloud config set run/region "${REGION}"

      - name: Verify Artifact Registry repository exists
        run: |
          set -euo pipefail
          gcloud artifacts repositories describe "${REPO}" \
            --location "${REGION}" \
            --project "${PROJECT_ID}" >/dev/null

      - name: Verify Secret Manager secrets exist
        run: |
          set -euo pipefail
          gcloud secrets describe "${SB_SECRET_KEY_SECRET_NAME}" --project "${PROJECT_ID}" >/dev/null
          gcloud secrets describe "${HISTORY_SECRET_KEY_SECRET_NAME}" --project "${PROJECT_ID}" >/dev/null
          for optional_secret in \
            "${DROPBOX_APP_KEY_SECRET_NAME:-}" \
            "${DROPBOX_APP_SECRET_SECRET_NAME:-}" \
            "${DROPBOX_REFRESH_TOKEN_SECRET_NAME:-}"; do
            if [ -n "${optional_secret}" ]; then
              gcloud secrets describe "${optional_secret}" --project "${PROJECT_ID}" >/dev/null
            fi
          done

      - name: Configure Docker auth for Artifact Registry
        run: |
          set -euo pipefail
          gcloud auth configure-docker "${REGION}-docker.pkg.dev" --quiet

      - name: Build and push image
        id: build
        run: |
          set -euo pipefail
          IMAGE="${REGION}-docker.pkg.dev/${PROJECT_ID}/${REPO}/uk-aq-ingestdb-prune:${GITHUB_SHA::12}"
          echo "image=${IMAGE}" >> "${GITHUB_OUTPUT}"
          docker build -t "${IMAGE}" .
          docker push "${IMAGE}"

      - name: Build Cloud Run env and secret args
        id: cloudrun_args
        run: |
          set -euo pipefail

          env_updates=()
          env_updates+=("SUPABASE_URL=${SUPABASE_URL}")
          env_updates+=("HISTORY_SUPABASE_URL=${HISTORY_SUPABASE_URL}")
          env_updates+=("INGESTDB_PRUNE_DRY_RUN=${INGESTDB_PRUNE_DRY_RUN}")
          env_updates+=("INGESTDB_RETENTION_DAYS=${INGESTDB_RETENTION_DAYS}")
          env_updates+=("MAX_HOURS_PER_RUN=${MAX_HOURS_PER_RUN}")
          env_updates+=("DELETE_BATCH_SIZE=${DELETE_BATCH_SIZE}")
          env_updates+=("MAX_DELETE_BATCHES_PER_HOUR=${MAX_DELETE_BATCHES_PER_HOUR}")
          env_updates+=("REPAIR_ONE_MISMATCH_BUCKET=${REPAIR_ONE_MISMATCH_BUCKET}")
          env_updates+=("REPAIR_BUCKET_OUTBOX_CHUNK_SIZE=${REPAIR_BUCKET_OUTBOX_CHUNK_SIZE}")
          env_updates+=("FLUSH_CLAIM_BATCH_LIMIT=${FLUSH_CLAIM_BATCH_LIMIT}")
          env_updates+=("MAX_FLUSH_BATCHES=${MAX_FLUSH_BATCHES}")
          if [ -n "${UK_AQ_DROPBOX_ROOT:-}" ]; then
            env_updates+=("UK_AQ_DROPBOX_ROOT=${UK_AQ_DROPBOX_ROOT}")
          fi
          if [ -n "${UK_AIR_ERROR_DROPBOX_FOLDER:-}" ]; then
            env_updates+=("UK_AIR_ERROR_DROPBOX_FOLDER=${UK_AIR_ERROR_DROPBOX_FOLDER}")
          fi
          if [ -n "${UK_AIR_ERROR_DROPBOX_ALLOWED_SUPABASE_URL:-}" ]; then
            env_updates+=("UK_AIR_ERROR_DROPBOX_ALLOWED_SUPABASE_URL=${UK_AIR_ERROR_DROPBOX_ALLOWED_SUPABASE_URL}")
          fi

          env_file="${RUNNER_TEMP}/uk-aq-ingestdb-prune-env.yaml"
          : > "${env_file}"
          for kv in "${env_updates[@]}"; do
            key="${kv%%=*}"
            value="${kv#*=}"
            python3 -c 'import json,sys; f=open(sys.argv[1], "a", encoding="utf-8"); print(f"{sys.argv[2]}: {json.dumps(sys.argv[3])}", file=f); f.close()' \
              "${env_file}" "${key}" "${value}"
          done

          secret_updates=()
          secret_updates+=("SB_SECRET_KEY=${SB_SECRET_KEY_SECRET_NAME}:latest")
          secret_updates+=("HISTORY_SECRET_KEY=${HISTORY_SECRET_KEY_SECRET_NAME}:latest")
          if [ -n "${DROPBOX_APP_KEY_SECRET_NAME:-}" ]; then
            secret_updates+=("DROPBOX_APP_KEY=${DROPBOX_APP_KEY_SECRET_NAME}:latest")
          fi
          if [ -n "${DROPBOX_APP_SECRET_SECRET_NAME:-}" ]; then
            secret_updates+=("DROPBOX_APP_SECRET=${DROPBOX_APP_SECRET_SECRET_NAME}:latest")
          fi
          if [ -n "${DROPBOX_REFRESH_TOKEN_SECRET_NAME:-}" ]; then
            secret_updates+=("DROPBOX_REFRESH_TOKEN=${DROPBOX_REFRESH_TOKEN_SECRET_NAME}:latest")
          fi
          secret_csv="$(IFS=,; echo "${secret_updates[*]}")"

          echo "env_file=${env_file}" >> "${GITHUB_OUTPUT}"
          echo "secret_csv=${secret_csv}" >> "${GITHUB_OUTPUT}"

      - name: Deploy Cloud Run service
        run: |
          set -euo pipefail
          gcloud run deploy "${SERVICE_NAME}" \
            --region "${REGION}" \
            --image "${{ steps.build.outputs.image }}" \
            --service-account "${{ steps.service_accounts.outputs.runtime_sa }}" \
            --timeout "${SERVICE_TIMEOUT_SECONDS}" \
            --cpu "${SERVICE_CPU}" \
            --memory "${SERVICE_MEMORY}" \
            --concurrency "${SERVICE_CONCURRENCY}" \
            --max-instances "${SERVICE_MAX_INSTANCES}" \
            --min-instances "${SERVICE_MIN_INSTANCES}" \
            --labels "job_name=${RESOURCE_LABEL},service_name=${SERVICE_NAME}" \
            --no-allow-unauthenticated \
            --env-vars-file "${{ steps.cloudrun_args.outputs.env_file }}" \
            --set-secrets "${{ steps.cloudrun_args.outputs.secret_csv }}"

      - name: Verify service labels
        run: |
          set -euo pipefail
          labels_json="$(gcloud run services describe "${SERVICE_NAME}" --region "${REGION}" --format="json(metadata.labels)")"
          job_name_label="$(python3 -c 'import json,sys; print(((json.loads(sys.stdin.read()) or {}).get("metadata", {}) or {}).get("labels", {}).get("job_name", ""))' <<<"${labels_json}")"
          if [ "${job_name_label}" != "${RESOURCE_LABEL}" ]; then
            echo "job_name label mismatch: expected ${RESOURCE_LABEL}, got ${job_name_label:-<missing>}"
            exit 1
          fi

      - name: Ensure Cloud Scheduler trigger exists
        run: |
          set -euo pipefail

          enabled="$(printf '%s' "${SCHEDULER_ENABLED}" | tr '[:upper:]' '[:lower:]')"
          if [ "${enabled}" != "true" ] && [ "${enabled}" != "1" ] && [ "${enabled}" != "yes" ]; then
            echo "Skipping scheduler trigger management (SCHEDULER_ENABLED=${SCHEDULER_ENABLED})."
            exit 0
          fi

          scheduler_sa="${{ steps.service_accounts.outputs.scheduler_sa }}"
          service_uri="$(gcloud run services describe "${SERVICE_NAME}" --region "${REGION}" --format='value(status.url)')"
          scheduler_uri="${service_uri}/run"
          scheduler_body='{}'

          gcloud run services add-iam-policy-binding "${SERVICE_NAME}" \
            --region "${REGION}" \
            --member "serviceAccount:${scheduler_sa}" \
            --role "roles/run.invoker" >/dev/null

          if gcloud scheduler jobs describe "${SCHEDULER_JOB_NAME}" --location "${REGION}" >/dev/null 2>&1; then
            gcloud scheduler jobs update http "${SCHEDULER_JOB_NAME}" \
              --location "${REGION}" \
              --schedule "${SCHEDULER_CRON}" \
              --time-zone "${SCHEDULER_TIMEZONE}" \
              --uri "${scheduler_uri}" \
              --http-method POST \
              --update-headers "Content-Type=application/json" \
              --message-body "${scheduler_body}" \
              --oidc-service-account-email "${scheduler_sa}" \
              --oidc-token-audience "${service_uri}" \
              --max-retry-attempts "${SCHEDULER_MAX_RETRY_ATTEMPTS}"
            echo "Updated Cloud Scheduler job: ${SCHEDULER_JOB_NAME}"
          else
            gcloud scheduler jobs create http "${SCHEDULER_JOB_NAME}" \
              --location "${REGION}" \
              --schedule "${SCHEDULER_CRON}" \
              --time-zone "${SCHEDULER_TIMEZONE}" \
              --uri "${scheduler_uri}" \
              --http-method POST \
              --headers "Content-Type=application/json" \
              --message-body "${scheduler_body}" \
              --oidc-service-account-email "${scheduler_sa}" \
              --oidc-token-audience "${service_uri}" \
              --max-retry-attempts "${SCHEDULER_MAX_RETRY_ATTEMPTS}"
            echo "Created Cloud Scheduler job: ${SCHEDULER_JOB_NAME}"
          fi

      - name: Show deployed service
        run: |
          set -euo pipefail
          gcloud run services describe "${SERVICE_NAME}" \
            --region "${REGION}" \
            --format="yaml(metadata.name,metadata.labels,spec.template.spec.serviceAccountName,spec.template.spec.containerConcurrency,spec.template.spec.containers[0].image,spec.template.spec.containers[0].resources,status.url)"
