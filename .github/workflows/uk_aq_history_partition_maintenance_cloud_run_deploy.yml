name: Deploy History Partition Maintenance Cloud Run Service

on:
  push:
    branches:
      - main
    paths:
      - "src/history_partition_maintenance_service.mjs"
      - "Dockerfile"
      - "package.json"
      - "package-lock.json"
      - ".github/workflows/uk_aq_history_partition_maintenance_cloud_run_deploy.yml"
  workflow_dispatch:

concurrency:
  group: uk-aq-history-partition-maintenance-service-deploy
  cancel-in-progress: false

jobs:
  deploy-history-partition-maintenance-service:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write
    env:
      PROJECT_ID: ${{ vars.GCP_PROJECT_ID }}
      REGION: ${{ vars.GCP_REGION || 'europe-west2' }}
      REPO: ${{ vars.GCP_ARTIFACT_REPO || 'uk-aq' }}
      SERVICE_NAME: ${{ vars.HISTORY_PARTITION_MAINTENANCE_SERVICE_NAME || 'uk-aq-history-partition-maintenance-service' }}
      RESOURCE_LABEL: uk-aq-history-partition-maintenance-service
      HISTORY_PARTITION_MAINTENANCE_RUNTIME_SERVICE_ACCOUNT: ${{ vars.HISTORY_PARTITION_MAINTENANCE_RUNTIME_SERVICE_ACCOUNT || '' }}
      SERVICE_TIMEOUT_SECONDS: ${{ vars.HISTORY_PARTITION_MAINTENANCE_SERVICE_TIMEOUT_SECONDS || '900' }}
      SERVICE_CPU: ${{ vars.HISTORY_PARTITION_MAINTENANCE_SERVICE_CPU || '1' }}
      SERVICE_MEMORY: ${{ vars.HISTORY_PARTITION_MAINTENANCE_SERVICE_MEMORY || '256Mi' }}
      SERVICE_CONCURRENCY: ${{ vars.HISTORY_PARTITION_MAINTENANCE_SERVICE_CONCURRENCY || '1' }}
      SERVICE_MAX_INSTANCES: ${{ vars.HISTORY_PARTITION_MAINTENANCE_SERVICE_MAX_INSTANCES || '1' }}
      SERVICE_MIN_INSTANCES: ${{ vars.HISTORY_PARTITION_MAINTENANCE_SERVICE_MIN_INSTANCES || '0' }}
      SCHEDULER_ENABLED: ${{ vars.HISTORY_PARTITION_MAINTENANCE_SCHEDULER_ENABLED || 'true' }}
      SCHEDULER_JOB_NAME: ${{ vars.HISTORY_PARTITION_MAINTENANCE_SCHEDULER_JOB_NAME || 'uk-aq-history-partition-maintenance-daily' }}
      SCHEDULER_CRON: ${{ vars.HISTORY_PARTITION_MAINTENANCE_SCHEDULER_CRON || '0 3 * * *' }}
      SCHEDULER_TIMEZONE: ${{ vars.HISTORY_PARTITION_MAINTENANCE_SCHEDULER_TIMEZONE || 'Etc/UTC' }}
      SCHEDULER_MAX_RETRY_ATTEMPTS: ${{ vars.HISTORY_PARTITION_MAINTENANCE_SCHEDULER_MAX_RETRY_ATTEMPTS || '0' }}
      HISTORY_PARTITION_MAINTENANCE_SCHEDULER_SERVICE_ACCOUNT: ${{ vars.HISTORY_PARTITION_MAINTENANCE_SCHEDULER_SERVICE_ACCOUNT || '' }}
      HISTORY_SUPABASE_URL: ${{ vars.HISTORY_SUPABASE_URL }}
      HISTORY_PARTITIONS_FUTURE_DAYS: ${{ vars.HISTORY_PARTITIONS_FUTURE_DAYS || '7' }}
      HISTORY_PARTITIONS_HOT_DAYS: ${{ vars.HISTORY_PARTITIONS_HOT_DAYS || '3' }}
      HISTORY_COMPLETE_LOCAL_DAYS: ${{ vars.HISTORY_COMPLETE_LOCAL_DAYS || '31' }}
      HISTORY_DEFAULT_TOP_N: ${{ vars.HISTORY_DEFAULT_TOP_N || '20' }}
      HISTORY_PARTITION_DROP_DRY_RUN: ${{ vars.HISTORY_PARTITION_DROP_DRY_RUN || 'false' }}
      HISTORY_R2_ENDPOINT: ${{ vars.HISTORY_R2_ENDPOINT || '' }}
      HISTORY_R2_BUCKET: ${{ vars.HISTORY_R2_BUCKET || '' }}
      HISTORY_R2_REGION: ${{ vars.HISTORY_R2_REGION || 'auto' }}
      HISTORY_R2_OBSERVATIONS_PREFIX: ${{ vars.HISTORY_R2_OBSERVATIONS_PREFIX || 'uk_aq_history/observations' }}
      UK_AQ_DROPBOX_ROOT: ${{ vars.UK_AQ_DROPBOX_ROOT || '' }}
      UK_AQ_HISTORY_PARTITION_DROPBOX_FOLDER: ${{ vars.UK_AQ_HISTORY_PARTITION_DROPBOX_FOLDER || '/history_partition_maintenance' }}
      UK_AIR_ERROR_DROPBOX_ALLOWED_SUPABASE_URL: ${{ vars.UK_AIR_ERROR_DROPBOX_ALLOWED_SUPABASE_URL || '' }}
      HISTORY_SECRET_KEY_SECRET_NAME: ${{ vars.HISTORY_SECRET_KEY_SECRET_NAME || 'HISTORY_SECRET_KEY' }}
      HISTORY_R2_ACCESS_KEY_ID_SECRET_NAME: ${{ vars.HISTORY_R2_ACCESS_KEY_ID_SECRET_NAME || '' }}
      HISTORY_R2_SECRET_ACCESS_KEY_SECRET_NAME: ${{ vars.HISTORY_R2_SECRET_ACCESS_KEY_SECRET_NAME || '' }}
      DROPBOX_APP_KEY_SECRET_NAME: ${{ vars.DROPBOX_APP_KEY_SECRET_NAME || '' }}
      DROPBOX_APP_SECRET_SECRET_NAME: ${{ vars.DROPBOX_APP_SECRET_SECRET_NAME || '' }}
      DROPBOX_REFRESH_TOKEN_SECRET_NAME: ${{ vars.DROPBOX_REFRESH_TOKEN_SECRET_NAME || '' }}
      GCP_WORKLOAD_IDENTITY_PROVIDER: ${{ vars.GCP_WORKLOAD_IDENTITY_PROVIDER }}
      GCP_SERVICE_ACCOUNT: ${{ vars.GCP_SERVICE_ACCOUNT }}
      GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Validate required project config
        run: |
          set -euo pipefail
          if [ -z "${PROJECT_ID:-}" ]; then
            echo "Missing required repo variable: GCP_PROJECT_ID"
            exit 1
          fi
          if [ -z "${HISTORY_SUPABASE_URL:-}" ]; then
            echo "Missing required repo variable: HISTORY_SUPABASE_URL"
            exit 1
          fi

      - name: Resolve runtime and scheduler service accounts
        id: service_accounts
        run: |
          set -euo pipefail

          runtime_sa="${HISTORY_PARTITION_MAINTENANCE_RUNTIME_SERVICE_ACCOUNT:-}"
          if [ -z "${runtime_sa}" ]; then
            runtime_sa="uk-aq-ops-job@${PROJECT_ID}.iam.gserviceaccount.com"
          fi
          if [[ "${runtime_sa}" == *"-compute@developer.gserviceaccount.com" ]]; then
            echo "Invalid runtime service account: default compute service account is not allowed."
            exit 1
          fi

          scheduler_sa="${HISTORY_PARTITION_MAINTENANCE_SCHEDULER_SERVICE_ACCOUNT:-${runtime_sa}}"
          if [[ "${scheduler_sa}" == *"-compute@developer.gserviceaccount.com" ]]; then
            echo "Invalid scheduler service account: default compute service account is not allowed."
            exit 1
          fi

          echo "runtime_sa=${runtime_sa}" >> "${GITHUB_OUTPUT}"
          echo "scheduler_sa=${scheduler_sa}" >> "${GITHUB_OUTPUT}"

      - name: Validate Google auth config
        run: |
          set -euo pipefail
          if [ -n "${GCP_WORKLOAD_IDENTITY_PROVIDER:-}" ] && [ -n "${GCP_SERVICE_ACCOUNT:-}" ]; then
            echo "Using Workload Identity Federation auth via existing deploy SA."
          elif [ -n "${GCP_SA_KEY:-}" ]; then
            echo "Using Service Account key auth fallback."
          else
            echo "Missing Google auth config."
            echo "Set either:"
            echo "1) GCP_WORKLOAD_IDENTITY_PROVIDER + GCP_SERVICE_ACCOUNT"
            echo "or"
            echo "2) GCP_SA_KEY"
            exit 1
          fi

      - name: Authenticate to Google Cloud (WIF)
        if: ${{ env.GCP_WORKLOAD_IDENTITY_PROVIDER != '' && env.GCP_SERVICE_ACCOUNT != '' }}
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: ${{ env.GCP_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ env.GCP_SERVICE_ACCOUNT }}

      - name: Authenticate to Google Cloud (Service Account Key fallback)
        if: ${{ env.GCP_WORKLOAD_IDENTITY_PROVIDER == '' || env.GCP_SERVICE_ACCOUNT == '' }}
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ env.GCP_SA_KEY }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v2

      - name: Configure gcloud project
        run: |
          set -euo pipefail
          gcloud config set project "${PROJECT_ID}"
          gcloud config set run/region "${REGION}"

      - name: Verify Artifact Registry repository exists
        run: |
          set -euo pipefail
          gcloud artifacts repositories describe "${REPO}" \
            --location "${REGION}" \
            --project "${PROJECT_ID}" >/dev/null

      - name: Verify Secret Manager secrets exist
        run: |
          set -euo pipefail
          gcloud secrets describe "${HISTORY_SECRET_KEY_SECRET_NAME}" --project "${PROJECT_ID}" >/dev/null
          for optional_secret in \
            "${HISTORY_R2_ACCESS_KEY_ID_SECRET_NAME:-}" \
            "${HISTORY_R2_SECRET_ACCESS_KEY_SECRET_NAME:-}" \
            "${DROPBOX_APP_KEY_SECRET_NAME:-}" \
            "${DROPBOX_APP_SECRET_SECRET_NAME:-}" \
            "${DROPBOX_REFRESH_TOKEN_SECRET_NAME:-}"; do
            if [ -n "${optional_secret}" ]; then
              gcloud secrets describe "${optional_secret}" --project "${PROJECT_ID}" >/dev/null
            fi
          done

      - name: Configure Docker auth for Artifact Registry
        run: |
          set -euo pipefail
          gcloud auth configure-docker "${REGION}-docker.pkg.dev" --quiet

      - name: Build and push image
        id: build
        run: |
          set -euo pipefail
          IMAGE="${REGION}-docker.pkg.dev/${PROJECT_ID}/${REPO}/uk-aq-history-partition-maintenance-service:${GITHUB_SHA::12}"
          echo "image=${IMAGE}" >> "${GITHUB_OUTPUT}"
          docker build -t "${IMAGE}" .
          docker push "${IMAGE}"

      - name: Build Cloud Run env and secret args
        id: cloudrun_args
        run: |
          set -euo pipefail

          env_updates=()
          env_updates+=("HISTORY_SUPABASE_URL=${HISTORY_SUPABASE_URL}")
          env_updates+=("HISTORY_PARTITIONS_FUTURE_DAYS=${HISTORY_PARTITIONS_FUTURE_DAYS}")
          env_updates+=("HISTORY_PARTITIONS_HOT_DAYS=${HISTORY_PARTITIONS_HOT_DAYS}")
          env_updates+=("HISTORY_COMPLETE_LOCAL_DAYS=${HISTORY_COMPLETE_LOCAL_DAYS}")
          env_updates+=("HISTORY_DEFAULT_TOP_N=${HISTORY_DEFAULT_TOP_N}")
          env_updates+=("HISTORY_PARTITION_DROP_DRY_RUN=${HISTORY_PARTITION_DROP_DRY_RUN}")
          if [ -n "${HISTORY_R2_ENDPOINT:-}" ]; then
            env_updates+=("HISTORY_R2_ENDPOINT=${HISTORY_R2_ENDPOINT}")
          fi
          if [ -n "${HISTORY_R2_BUCKET:-}" ]; then
            env_updates+=("HISTORY_R2_BUCKET=${HISTORY_R2_BUCKET}")
          fi
          if [ -n "${HISTORY_R2_REGION:-}" ]; then
            env_updates+=("HISTORY_R2_REGION=${HISTORY_R2_REGION}")
          fi
          if [ -n "${HISTORY_R2_OBSERVATIONS_PREFIX:-}" ]; then
            env_updates+=("HISTORY_R2_OBSERVATIONS_PREFIX=${HISTORY_R2_OBSERVATIONS_PREFIX}")
          fi
          if [ -n "${UK_AQ_DROPBOX_ROOT:-}" ]; then
            env_updates+=("UK_AQ_DROPBOX_ROOT=${UK_AQ_DROPBOX_ROOT}")
          fi
          if [ -n "${UK_AQ_HISTORY_PARTITION_DROPBOX_FOLDER:-}" ]; then
            env_updates+=("UK_AQ_HISTORY_PARTITION_DROPBOX_FOLDER=${UK_AQ_HISTORY_PARTITION_DROPBOX_FOLDER}")
          fi
          if [ -n "${UK_AIR_ERROR_DROPBOX_ALLOWED_SUPABASE_URL:-}" ]; then
            env_updates+=("UK_AIR_ERROR_DROPBOX_ALLOWED_SUPABASE_URL=${UK_AIR_ERROR_DROPBOX_ALLOWED_SUPABASE_URL}")
          fi

          env_file="${RUNNER_TEMP}/uk-aq-history-partition-maintenance-env.yaml"
          : > "${env_file}"
          for kv in "${env_updates[@]}"; do
            key="${kv%%=*}"
            value="${kv#*=}"
            python3 -c 'import json,sys; f=open(sys.argv[1], "a", encoding="utf-8"); print(f"{sys.argv[2]}: {json.dumps(sys.argv[3])}", file=f); f.close()' \
              "${env_file}" "${key}" "${value}"
          done

          secret_updates=()
          secret_updates+=("HISTORY_SECRET_KEY=${HISTORY_SECRET_KEY_SECRET_NAME}:latest")
          if [ -n "${HISTORY_R2_ACCESS_KEY_ID_SECRET_NAME:-}" ]; then
            secret_updates+=("HISTORY_R2_ACCESS_KEY_ID=${HISTORY_R2_ACCESS_KEY_ID_SECRET_NAME}:latest")
          fi
          if [ -n "${HISTORY_R2_SECRET_ACCESS_KEY_SECRET_NAME:-}" ]; then
            secret_updates+=("HISTORY_R2_SECRET_ACCESS_KEY=${HISTORY_R2_SECRET_ACCESS_KEY_SECRET_NAME}:latest")
          fi
          if [ -n "${DROPBOX_APP_KEY_SECRET_NAME:-}" ]; then
            secret_updates+=("DROPBOX_APP_KEY=${DROPBOX_APP_KEY_SECRET_NAME}:latest")
          fi
          if [ -n "${DROPBOX_APP_SECRET_SECRET_NAME:-}" ]; then
            secret_updates+=("DROPBOX_APP_SECRET=${DROPBOX_APP_SECRET_SECRET_NAME}:latest")
          fi
          if [ -n "${DROPBOX_REFRESH_TOKEN_SECRET_NAME:-}" ]; then
            secret_updates+=("DROPBOX_REFRESH_TOKEN=${DROPBOX_REFRESH_TOKEN_SECRET_NAME}:latest")
          fi
          secret_csv="$(IFS=,; echo "${secret_updates[*]}")"

          echo "env_file=${env_file}" >> "${GITHUB_OUTPUT}"
          echo "secret_csv=${secret_csv}" >> "${GITHUB_OUTPUT}"

      - name: Deploy Cloud Run service
        run: |
          set -euo pipefail
          gcloud run deploy "${SERVICE_NAME}" \
            --region "${REGION}" \
            --image "${{ steps.build.outputs.image }}" \
            --command "node" \
            --args "src/history_partition_maintenance_service.mjs" \
            --service-account "${{ steps.service_accounts.outputs.runtime_sa }}" \
            --timeout "${SERVICE_TIMEOUT_SECONDS}" \
            --cpu "${SERVICE_CPU}" \
            --memory "${SERVICE_MEMORY}" \
            --concurrency "${SERVICE_CONCURRENCY}" \
            --max-instances "${SERVICE_MAX_INSTANCES}" \
            --min-instances "${SERVICE_MIN_INSTANCES}" \
            --labels "job_name=${RESOURCE_LABEL},service_name=${SERVICE_NAME}" \
            --no-allow-unauthenticated \
            --env-vars-file "${{ steps.cloudrun_args.outputs.env_file }}" \
            --set-secrets "${{ steps.cloudrun_args.outputs.secret_csv }}"

      - name: Verify service labels
        run: |
          set -euo pipefail
          labels_json="$(gcloud run services describe "${SERVICE_NAME}" --region "${REGION}" --format="json(metadata.labels)")"
          job_name_label="$(python3 -c 'import json,sys; print(((json.loads(sys.stdin.read()) or {}).get("metadata", {}) or {}).get("labels", {}).get("job_name", ""))' <<<"${labels_json}")"
          if [ "${job_name_label}" != "${RESOURCE_LABEL}" ]; then
            echo "job_name label mismatch: expected ${RESOURCE_LABEL}, got ${job_name_label:-<missing>}"
            exit 1
          fi

      - name: Ensure Cloud Scheduler trigger exists
        run: |
          set -euo pipefail

          enabled="$(printf '%s' "${SCHEDULER_ENABLED}" | tr '[:upper:]' '[:lower:]')"
          if [ "${enabled}" != "true" ] && [ "${enabled}" != "1" ] && [ "${enabled}" != "yes" ]; then
            echo "Skipping scheduler trigger management (SCHEDULER_ENABLED=${SCHEDULER_ENABLED})."
            exit 0
          fi

          scheduler_sa="${{ steps.service_accounts.outputs.scheduler_sa }}"
          service_uri="$(gcloud run services describe "${SERVICE_NAME}" --region "${REGION}" --format='value(status.url)')"
          scheduler_uri="${service_uri}/run"
          scheduler_body='{}'

          gcloud run services add-iam-policy-binding "${SERVICE_NAME}" \
            --region "${REGION}" \
            --member "serviceAccount:${scheduler_sa}" \
            --role "roles/run.invoker" >/dev/null

          if gcloud scheduler jobs describe "${SCHEDULER_JOB_NAME}" --location "${REGION}" >/dev/null 2>&1; then
            gcloud scheduler jobs update http "${SCHEDULER_JOB_NAME}" \
              --location "${REGION}" \
              --schedule "${SCHEDULER_CRON}" \
              --time-zone "${SCHEDULER_TIMEZONE}" \
              --uri "${scheduler_uri}" \
              --http-method POST \
              --update-headers "Content-Type=application/json" \
              --message-body "${scheduler_body}" \
              --oidc-service-account-email "${scheduler_sa}" \
              --oidc-token-audience "${service_uri}" \
              --max-retry-attempts "${SCHEDULER_MAX_RETRY_ATTEMPTS}"
            echo "Updated Cloud Scheduler job: ${SCHEDULER_JOB_NAME}"
          else
            gcloud scheduler jobs create http "${SCHEDULER_JOB_NAME}" \
              --location "${REGION}" \
              --schedule "${SCHEDULER_CRON}" \
              --time-zone "${SCHEDULER_TIMEZONE}" \
              --uri "${scheduler_uri}" \
              --http-method POST \
              --headers "Content-Type=application/json" \
              --message-body "${scheduler_body}" \
              --oidc-service-account-email "${scheduler_sa}" \
              --oidc-token-audience "${service_uri}" \
              --max-retry-attempts "${SCHEDULER_MAX_RETRY_ATTEMPTS}"
            echo "Created Cloud Scheduler job: ${SCHEDULER_JOB_NAME}"
          fi

      - name: Show deployed service
        run: |
          set -euo pipefail
          gcloud run services describe "${SERVICE_NAME}" \
            --region "${REGION}" \
            --format="yaml(metadata.name,metadata.labels,spec.template.spec.serviceAccountName,spec.template.spec.containerConcurrency,spec.template.spec.containers[0].image,spec.template.spec.containers[0].resources,status.url)"
