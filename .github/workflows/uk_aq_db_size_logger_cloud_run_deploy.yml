name: Deploy DB Size Logger Cloud Run Service

on:
  push:
    branches:
      - main
    paths:
      - "workers/uk_aq_db_size_logger_cloud_run/**"
      - "scripts/gcp/uk_aq_secret_upsert_if_changed.sh"
      - ".github/workflows/uk_aq_db_size_logger_cloud_run_deploy.yml"
  workflow_dispatch:

concurrency:
  group: uk-aq-db-size-logger-deploy
  cancel-in-progress: false

jobs:
  deploy-db-size-logger-service:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write
    env:
      PROJECT_ID: ${{ vars.GCP_PROJECT_ID }}
      REGION: ${{ vars.GCP_REGION || 'europe-west2' }}
      REPO: ${{ vars.GCP_ARTIFACT_REPO || 'uk-aq' }}
      SERVICE_NAME: ${{ vars.GCP_DB_SIZE_LOGGER_SERVICE_NAME || vars.DB_SIZE_LOGGER_SERVICE_NAME || 'uk-aq-db-size-logger' }}
      DB_SIZE_LOGGER_SERVICE_ACCOUNT: ${{ vars.GCP_DB_SIZE_LOGGER_SERVICE_ACCOUNT || vars.GCP_OPS_JOB_SERVICE_ACCOUNT || vars.DB_SIZE_LOGGER_SERVICE_ACCOUNT || '' }}
      RESOURCE_LABEL: uk-aq-db-size-logger-service
      SERVICE_TIMEOUT_SECONDS: ${{ vars.GCP_DB_SIZE_LOGGER_SERVICE_TIMEOUT_SECONDS || vars.DB_SIZE_LOGGER_SERVICE_TIMEOUT_SECONDS || '300' }}
      SERVICE_CPU: ${{ vars.GCP_DB_SIZE_LOGGER_SERVICE_CPU || vars.DB_SIZE_LOGGER_SERVICE_CPU || '0.25' }}
      SERVICE_MEMORY: ${{ vars.GCP_DB_SIZE_LOGGER_SERVICE_MEMORY || vars.DB_SIZE_LOGGER_SERVICE_MEMORY || '256Mi' }}
      SERVICE_CONCURRENCY: ${{ vars.GCP_DB_SIZE_LOGGER_SERVICE_CONCURRENCY || vars.DB_SIZE_LOGGER_SERVICE_CONCURRENCY || '1' }}
      SERVICE_MAX_INSTANCES: ${{ vars.GCP_DB_SIZE_LOGGER_SERVICE_MAX_INSTANCES || vars.DB_SIZE_LOGGER_SERVICE_MAX_INSTANCES || '1' }}
      SERVICE_MIN_INSTANCES: ${{ vars.GCP_DB_SIZE_LOGGER_SERVICE_MIN_INSTANCES || vars.DB_SIZE_LOGGER_SERVICE_MIN_INSTANCES || '0' }}
      SCHEDULER_ENABLED: ${{ vars.GCP_DB_SIZE_LOGGER_SCHEDULER_ENABLED || vars.DB_SIZE_LOGGER_SCHEDULER_ENABLED || 'true' }}
      SCHEDULER_JOB_NAME: ${{ vars.GCP_DB_SIZE_LOGGER_SCHEDULER_JOB_NAME || vars.DB_SIZE_LOGGER_SCHEDULER_JOB_NAME || 'uk-aq-db-size-logger-hourly' }}
      SCHEDULER_CRON: ${{ vars.GCP_DB_SIZE_LOGGER_SCHEDULER_CRON || vars.DB_SIZE_LOGGER_SCHEDULER_CRON || '0 * * * *' }}
      SCHEDULER_TIMEZONE: ${{ vars.GCP_DB_SIZE_LOGGER_SCHEDULER_TIMEZONE || vars.DB_SIZE_LOGGER_SCHEDULER_TIMEZONE || 'Etc/UTC' }}
      SCHEDULER_MAX_RETRY_ATTEMPTS: ${{ vars.GCP_DB_SIZE_LOGGER_SCHEDULER_MAX_RETRY_ATTEMPTS || vars.DB_SIZE_LOGGER_SCHEDULER_MAX_RETRY_ATTEMPTS || '0' }}
      SCHEDULER_SERVICE_ACCOUNT: ${{ vars.GCP_DB_SIZE_LOGGER_SCHEDULER_SERVICE_ACCOUNT || vars.DB_SIZE_LOGGER_SCHEDULER_SERVICE_ACCOUNT || '' }}
      SUPABASE_URL: ${{ vars.SUPABASE_URL }}
      HISTORY_SUPABASE_URL: ${{ vars.HISTORY_SUPABASE_URL }}
      AGGDAILY_SUPABASE_URL: ${{ vars.AGGDAILY_SUPABASE_URL }}
      UK_AQ_PUBLIC_SCHEMA: ${{ vars.UK_AQ_PUBLIC_SCHEMA || 'uk_aq_public' }}
      UK_AQ_DB_SIZE_RPC: ${{ vars.UK_AQ_DB_SIZE_RPC || 'uk_aq_rpc_database_size_bytes' }}
      UK_AQ_DB_SIZE_UPSERT_RPC: ${{ vars.UK_AQ_DB_SIZE_UPSERT_RPC || 'uk_aq_rpc_db_size_metric_upsert' }}
      UK_AQ_DB_SIZE_CLEANUP_RPC: ${{ vars.UK_AQ_DB_SIZE_CLEANUP_RPC || 'uk_aq_rpc_db_size_metric_cleanup' }}
      UK_AQ_DB_SIZE_RETENTION_DAYS: ${{ vars.UK_AQ_DB_SIZE_RETENTION_DAYS || '120' }}
      UK_AQ_DB_SIZE_RPC_RETRIES: ${{ vars.UK_AQ_DB_SIZE_RPC_RETRIES || '3' }}
      UK_AQ_INGEST_DB_LABEL: ${{ vars.UK_AQ_INGEST_DB_LABEL || 'ingestdb' }}
      UK_AQ_HISTORY_DB_LABEL: ${{ vars.UK_AQ_HISTORY_DB_LABEL || 'historydb' }}
      UK_AQ_AGGDAILY_DB_LABEL: ${{ vars.UK_AQ_AGGDAILY_DB_LABEL || 'aggdailydb' }}
      SB_SECRET_KEY: ${{ secrets.SB_SECRET_KEY }}
      HISTORY_SECRET_KEY: ${{ secrets.HISTORY_SECRET_KEY }}
      AGGDAILY_SECRET_KEY: ${{ secrets.AGGDAILY_SECRET_KEY }}
      SB_SECRET_KEY_SECRET_NAME: ${{ vars.SB_SECRET_KEY_SECRET_NAME || 'SB_SECRET_KEY' }}
      HISTORY_SECRET_KEY_SECRET_NAME: ${{ vars.HISTORY_SECRET_KEY_SECRET_NAME || 'HISTORY_SECRET_KEY' }}
      AGGDAILY_SECRET_KEY_SECRET_NAME: ${{ vars.AGGDAILY_SECRET_KEY_SECRET_NAME || 'AGGDAILY_SECRET_KEY' }}
      GCP_WORKLOAD_IDENTITY_PROVIDER: ${{ vars.GCP_WORKLOAD_IDENTITY_PROVIDER }}
      GCP_SERVICE_ACCOUNT: ${{ vars.GCP_SERVICE_ACCOUNT }}
      GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Validate required project config
        run: |
          set -euo pipefail
          if [ -z "${PROJECT_ID:-}" ]; then
            echo "Missing required variable: GCP_PROJECT_ID"
            exit 1
          fi
          if [ -z "${DB_SIZE_LOGGER_SERVICE_ACCOUNT:-}" ]; then
            echo "Missing required service account setting: GCP_DB_SIZE_LOGGER_SERVICE_ACCOUNT (or GCP_OPS_JOB_SERVICE_ACCOUNT)"
            exit 1
          fi
          if [[ "${DB_SIZE_LOGGER_SERVICE_ACCOUNT}" == *"-compute@developer.gserviceaccount.com" ]]; then
            echo "Invalid runtime service account: default compute service account is not allowed."
            exit 1
          fi
          if [ -z "${SUPABASE_URL:-}" ]; then
            echo "Missing required variable: SUPABASE_URL"
            exit 1
          fi
          if [ -z "${HISTORY_SUPABASE_URL:-}" ]; then
            echo "Missing required variable: HISTORY_SUPABASE_URL"
            exit 1
          fi
          if [ -n "${AGGDAILY_SUPABASE_URL:-}" ] && [ -z "${AGGDAILY_SECRET_KEY:-}" ]; then
            echo "AGGDAILY_SUPABASE_URL is set but AGGDAILY_SECRET_KEY is missing."
            exit 1
          fi

      - name: Validate Google auth config
        run: |
          set -euo pipefail
          if [ -n "${GCP_WORKLOAD_IDENTITY_PROVIDER:-}" ] && [ -n "${GCP_SERVICE_ACCOUNT:-}" ]; then
            echo "Using Workload Identity Federation auth."
          elif [ -n "${GCP_SA_KEY:-}" ]; then
            echo "Using Service Account key auth."
          else
            echo "Missing Google auth config."
            echo "Set either:"
            echo "1) GCP_WORKLOAD_IDENTITY_PROVIDER + GCP_SERVICE_ACCOUNT"
            echo "or"
            echo "2) GCP_SA_KEY"
            exit 1
          fi

      - name: Authenticate to Google Cloud (WIF)
        if: ${{ env.GCP_WORKLOAD_IDENTITY_PROVIDER != '' && env.GCP_SERVICE_ACCOUNT != '' }}
        uses: google-github-actions/auth@c200f3691d83b41bf9bbd8638997a462592937ed
        with:
          workload_identity_provider: ${{ env.GCP_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ env.GCP_SERVICE_ACCOUNT }}

      - name: Authenticate to Google Cloud (Service Account Key fallback)
        if: ${{ env.GCP_WORKLOAD_IDENTITY_PROVIDER == '' || env.GCP_SERVICE_ACCOUNT == '' }}
        uses: google-github-actions/auth@c200f3691d83b41bf9bbd8638997a462592937ed
        with:
          credentials_json: ${{ env.GCP_SA_KEY }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@e427ad8a34f8676edf47cf7d7925499adf3eb74f

      - name: Configure gcloud project
        run: |
          set -euo pipefail
          gcloud config set project "${PROJECT_ID}"
          gcloud config set run/region "${REGION}"

      - name: Verify Artifact Registry repository exists
        run: |
          set -euo pipefail
          gcloud artifacts repositories describe "${REPO}" \
            --location "${REGION}" \
            --project "${PROJECT_ID}" >/dev/null

      - name: Configure Docker auth for Artifact Registry
        run: |
          set -euo pipefail
          gcloud auth configure-docker "${REGION}-docker.pkg.dev" --quiet

      - name: Build and push db size logger image
        id: build
        run: |
          set -euo pipefail
          IMAGE="${REGION}-docker.pkg.dev/${PROJECT_ID}/${REPO}/uk-aq-db-size-logger:${GITHUB_SHA::12}"
          echo "image=${IMAGE}" >> "${GITHUB_OUTPUT}"
          docker build -f workers/uk_aq_db_size_logger_cloud_run/Dockerfile -t "${IMAGE}" .
          docker push "${IMAGE}"

      - name: Sync GitHub secrets to GCP Secret Manager
        run: |
          set -euo pipefail
          UPSERT_SCRIPT="scripts/gcp/uk_aq_secret_upsert_if_changed.sh"

          upsert_secret() {
            local secret_name="$1"
            local secret_value="$2"
            local required="${3:-0}"

            if [ -z "${secret_name}" ]; then
              if [ "${required}" = "1" ]; then
                echo "Missing required secret name."
                exit 1
              fi
              return 0
            fi

            if [ -z "${secret_value}" ] && [ "${required}" = "0" ]; then
              echo "Skipping optional secret ${secret_name} (no GitHub secret/variable value)."
              return 0
            fi

            printf '%s' "${secret_value}" | "${UPSERT_SCRIPT}" \
              --project "${PROJECT_ID}" \
              --secret "${secret_name}" \
              --required "${required}"
          }

          upsert_secret "${SB_SECRET_KEY_SECRET_NAME}" "${SB_SECRET_KEY}" 1
          upsert_secret "${HISTORY_SECRET_KEY_SECRET_NAME}" "${HISTORY_SECRET_KEY}" 1
          aggdaily_required=0
          if [ -n "${AGGDAILY_SUPABASE_URL:-}" ]; then
            aggdaily_required=1
          fi
          upsert_secret "${AGGDAILY_SECRET_KEY_SECRET_NAME}" "${AGGDAILY_SECRET_KEY}" "${aggdaily_required}"

      - name: Build Cloud Run env and secret args
        id: cloudrun_args
        run: |
          set -euo pipefail

          require_env() {
            local key="$1"
            local value="${!key:-}"
            if [ -z "${value}" ]; then
              echo "Missing required value for ${key}"
              exit 1
            fi
          }

          add_env() {
            local key="$1"
            local value="${!key:-}"
            if [ -n "${value}" ]; then
              env_updates+=("${key}=${value}")
            fi
          }

          add_secret_binding() {
            local env_key="$1"
            local secret_name="$2"
            local required="${3:-0}"
            if [ -z "${secret_name}" ]; then
              if [ "${required}" = "1" ]; then
                echo "Missing secret name for required binding ${env_key}"
                exit 1
              fi
              return 0
            fi
            if gcloud secrets describe "${secret_name}" --project "${PROJECT_ID}" >/dev/null 2>&1; then
              secret_updates+=("${env_key}=${secret_name}:latest")
              return 0
            fi
            if [ "${required}" = "1" ]; then
              echo "Required Secret Manager secret not found: ${secret_name}"
              exit 1
            fi
          }

          require_env SUPABASE_URL
          require_env HISTORY_SUPABASE_URL

          env_updates=()
          add_env SUPABASE_URL
          add_env HISTORY_SUPABASE_URL
          add_env AGGDAILY_SUPABASE_URL
          add_env UK_AQ_PUBLIC_SCHEMA
          add_env UK_AQ_DB_SIZE_RPC
          add_env UK_AQ_DB_SIZE_UPSERT_RPC
          add_env UK_AQ_DB_SIZE_CLEANUP_RPC
          add_env UK_AQ_DB_SIZE_RETENTION_DAYS
          add_env UK_AQ_DB_SIZE_RPC_RETRIES
          add_env UK_AQ_INGEST_DB_LABEL
          add_env UK_AQ_HISTORY_DB_LABEL
          add_env UK_AQ_AGGDAILY_DB_LABEL
          env_updates+=("GCP_PROJECT_ID=${PROJECT_ID}")

          secret_updates=()
          add_secret_binding "SB_SECRET_KEY" "${SB_SECRET_KEY_SECRET_NAME}" 1
          add_secret_binding "HISTORY_SECRET_KEY" "${HISTORY_SECRET_KEY_SECRET_NAME}" 1
          aggdaily_required=0
          if [ -n "${AGGDAILY_SUPABASE_URL:-}" ]; then
            aggdaily_required=1
          fi
          add_secret_binding "AGGDAILY_SECRET_KEY" "${AGGDAILY_SECRET_KEY_SECRET_NAME}" "${aggdaily_required}"

          env_file="${RUNNER_TEMP}/uk-aq-db-size-logger-env.yaml"
          : > "${env_file}"
          for kv in "${env_updates[@]}"; do
            key="${kv%%=*}"
            value="${kv#*=}"
            python3 -c 'import json,sys; f=open(sys.argv[1], "a", encoding="utf-8"); print(f"{sys.argv[2]}: {json.dumps(sys.argv[3])}", file=f); f.close()' \
              "${env_file}" "${key}" "${value}"
          done

          secret_csv="$(IFS=,; echo "${secret_updates[*]}")"
          echo "env_file=${env_file}" >> "${GITHUB_OUTPUT}"
          echo "secret_csv=${secret_csv}" >> "${GITHUB_OUTPUT}"

      - name: Deploy Cloud Run service
        run: |
          set -euo pipefail
          ENV_FILE='${{ steps.cloudrun_args.outputs.env_file }}'
          SECRET_CSV='${{ steps.cloudrun_args.outputs.secret_csv }}'

          CMD=(
            gcloud run deploy "${SERVICE_NAME}"
            --region "${REGION}"
            --image "${{ steps.build.outputs.image }}"
            --service-account "${DB_SIZE_LOGGER_SERVICE_ACCOUNT}"
            --timeout "${SERVICE_TIMEOUT_SECONDS}"
            --cpu "${SERVICE_CPU}"
            --memory "${SERVICE_MEMORY}"
            --concurrency "${SERVICE_CONCURRENCY}"
            --max-instances "${SERVICE_MAX_INSTANCES}"
            --min-instances "${SERVICE_MIN_INSTANCES}"
            --labels "job_name=${RESOURCE_LABEL}"
            --no-allow-unauthenticated
          )
          if [ -n "${ENV_FILE}" ]; then
            CMD+=(--env-vars-file "${ENV_FILE}")
          fi
          if [ -n "${SECRET_CSV}" ]; then
            CMD+=(--set-secrets "${SECRET_CSV}")
          fi
          "${CMD[@]}"

      - name: Verify service labels
        run: |
          set -euo pipefail
          labels_json="$(gcloud run services describe "${SERVICE_NAME}" --region "${REGION}" --format="json(metadata.labels)")"
          job_name_label="$(
            python3 -c 'import json,sys; print(((json.loads(sys.stdin.read()) or {}).get("metadata", {}) or {}).get("labels", {}).get("job_name", ""))' <<<"${labels_json}"
          )"
          if [ "${job_name_label}" != "${RESOURCE_LABEL}" ]; then
            echo "job_name label mismatch: expected ${RESOURCE_LABEL}, got ${job_name_label:-<missing>}"
            exit 1
          fi
          echo "Verified label job_name=${job_name_label}"

      - name: Ensure Cloud Scheduler trigger exists
        run: |
          set -euo pipefail

          enabled="$(printf '%s' "${SCHEDULER_ENABLED}" | tr '[:upper:]' '[:lower:]')"
          if [ "${enabled}" != "true" ] && [ "${enabled}" != "1" ] && [ "${enabled}" != "yes" ]; then
            echo "Skipping scheduler trigger management (SCHEDULER_ENABLED=${SCHEDULER_ENABLED})."
            exit 0
          fi

          scheduler_sa="${SCHEDULER_SERVICE_ACCOUNT:-${DB_SIZE_LOGGER_SERVICE_ACCOUNT}}"
          service_uri="$(gcloud run services describe "${SERVICE_NAME}" --region "${REGION}" --format='value(status.url)')"
          scheduler_body='{"trigger_mode":"scheduler"}'

          gcloud run services add-iam-policy-binding "${SERVICE_NAME}" \
            --region "${REGION}" \
            --member "serviceAccount:${scheduler_sa}" \
            --role "roles/run.invoker" >/dev/null

          if gcloud scheduler jobs describe "${SCHEDULER_JOB_NAME}" --location "${REGION}" >/dev/null 2>&1; then
            gcloud scheduler jobs update http "${SCHEDULER_JOB_NAME}" \
              --location "${REGION}" \
              --schedule "${SCHEDULER_CRON}" \
              --time-zone "${SCHEDULER_TIMEZONE}" \
              --uri "${service_uri}" \
              --http-method POST \
              --update-headers "Content-Type=application/json" \
              --message-body "${scheduler_body}" \
              --oidc-service-account-email "${scheduler_sa}" \
              --oidc-token-audience "${service_uri}" \
              --max-retry-attempts "${SCHEDULER_MAX_RETRY_ATTEMPTS}"
            echo "Updated Cloud Scheduler job: ${SCHEDULER_JOB_NAME}"
          else
            gcloud scheduler jobs create http "${SCHEDULER_JOB_NAME}" \
              --location "${REGION}" \
              --schedule "${SCHEDULER_CRON}" \
              --time-zone "${SCHEDULER_TIMEZONE}" \
              --uri "${service_uri}" \
              --http-method POST \
              --headers "Content-Type=application/json" \
              --message-body "${scheduler_body}" \
              --oidc-service-account-email "${scheduler_sa}" \
              --oidc-token-audience "${service_uri}" \
              --max-retry-attempts "${SCHEDULER_MAX_RETRY_ATTEMPTS}"
            echo "Created Cloud Scheduler job: ${SCHEDULER_JOB_NAME}"
          fi

      - name: Show deployed service
        run: |
          gcloud run services describe "${SERVICE_NAME}" \
            --region "${REGION}" \
            --format="yaml(metadata.name,metadata.labels,spec.template.spec.serviceAccountName,spec.template.spec.containerConcurrency,spec.template.spec.containers[0].image,spec.template.spec.containers[0].resources,status.url)"
